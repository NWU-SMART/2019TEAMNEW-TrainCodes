# -*- coding: utf-8 -*-
"""lstm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GX0Rqur8T45MSYhLU9MYWAbycfLH4-Fu
"""

# !pip install torch
# !pip install torchtext
# !python -m spacy download en # -m: run library module as a script(将模块当作脚本运行) # en: 英文模型数据


# K80 gpu for 12 hours
import torch
from torch import nn, optim
from torchtext import data, datasets

print('GPU:', torch.cuda.is_available())

torch.manual_seed(123)

# step 1 定义预处理方法Field

TEXT = data.Field(tokenize='spacy') # TEXT、LABEL皆为预处理方法
LABEL = data.LabelField(dtype=torch.float)

# step 2 定义数据集

train_data, test_data = datasets.IMDB.splits(TEXT, LABEL) # 此处会下载IMDB数据集

print('len of train data:', len(train_data))
print('len of test data:', len(test_data))

print(train_data.examples[15].text) # 打印文本样例
print(train_data.examples[15].label)

# step 3 建立词典

# word2vec, glove
TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d') # 此处会下载glove.6B
LABEL.build_vocab(train_data)

# step 4 构造iterator

batchsz = 30
device = torch.device('cuda')
train_iterator, test_iterator = data.BucketIterator.splits( # 相比Iterator,BucketIterator的功能较强大点,支持排序,动态padding等
    (train_data, test_data),
    batch_size = batchsz,
    device=device
)

# step 5 建立模型

class RNN(nn.Module):
    
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        """
        """
        super(RNN, self).__init__()
        
        # [0-10001] => [100]
        self.embedding = nn.Embedding(vocab_size, embedding_dim) # 创建embedding层：将10000维的词典降维成100维的词向量
        # [100] => [256]
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,
                           bidirectional=True, dropout=0.5)
        # [256*2] => [1]
        self.fc = nn.Linear(hidden_dim*2, 1)
        self.dropout = nn.Dropout(0.5)
        
        
    def forward(self, x):

        # [seq, b, 1] => [seq, b, 100]
        embedding = self.dropout(self.embedding(x)) # 调用embedding层：将1维单词转化成100维的词向量（查表）
        
        # output: [seq, b, hid_dim*2] # 因为是双向的lstm所以很多参数乘了2
        # hidden/h: [num_layers*2, b, hid_dim]
        # cell/c: [num_layers*2, b, hid_di]
        output, (hidden, cell) = self.lstm(embedding)
        
        # [num_layers*2, b, hid_dim] => 2 of [b, hid_dim] => [b, hid_dim*2]
        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1) # 取最后两个hidden元素，做一个concat，进行输出
        
        # [b, hid_dim*2] => [b, 1]
        hidden = self.dropout(hidden)
        out = self.fc(hidden)
        
        return out

rnn = RNN(len(TEXT.vocab), 100, 256) # vocab_size, embedding_dim, hidden_dim # 实例化

# 初始化embedding层的权重

pretrained_embedding = TEXT.vocab.vectors
print('pretrained_embedding:', pretrained_embedding.shape)
rnn.embedding.weight.data.copy_(pretrained_embedding)
print('embedding layer inited.')

# 优化器、loss、网络

optimizer = optim.Adam(rnn.parameters(), lr=1e-3)
criteon = nn.BCEWithLogitsLoss().to(device) # BCE: 二值交叉熵(Binary Cross Entropy)
# 计算target和output间的二值交叉熵，L(x,y)=x*logy+(1-x)*log(1-y)
# BCEWithLogitsLoss层包括了Sigmoid层和BCELoss层，数值计算稳定性更好
rnn.to(device)

import numpy as np

def binary_acc(preds, y):

    preds = torch.round(torch.sigmoid(preds))
    correct = torch.eq(preds, y).float()
    acc = correct.sum() / len(correct)
    return acc

# step 6 训练

def train(rnn, iterator, optimizer, criteon):
    
    avg_acc = []
    rnn.train()
    
    for i, batch in enumerate(iterator): # 这里的batch是核心数据，包括batch.text和batch.label
        
        # [seq, b, 1] => [b, 1] => [b]
        pred = rnn(batch.text).squeeze(1) # batch.text: [seq, b, 1]
        # criteon([b], [b])
        loss = criteon(pred, batch.label) # batch.label: [b] 每句话的输出就一个标量，它表示这句话里面pos的可能性大小
        acc = binary_acc(pred, batch.label).item()
        avg_acc.append(acc)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if i%10 == 0:
            print(i, acc)
        
    avg_acc = np.array(avg_acc).mean()
    print('avg acc:', avg_acc)
    

# step 7 测试

def eval(rnn, iterator, criteon):
    
    avg_acc = []
    
    rnn.eval()
    
    with torch.no_grad():
        for batch in iterator:

            # [b, 1] => [b]
            pred = rnn(batch.text).squeeze(1)

            #
            loss = criteon(pred, batch.label)

            acc = binary_acc(pred, batch.label).item()
            avg_acc.append(acc)
        
    avg_acc = np.array(avg_acc).mean()
    
    print('>>test:', avg_acc)

# 主程序

for epoch in range(10):
    
    eval(rnn, test_iterator, criteon)
    train(rnn, train_iterator, optimizer, criteon)